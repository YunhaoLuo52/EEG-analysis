{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGJOLZBMXygj",
        "outputId": "6b91e175-ca32-4bad-d3c3-33ff7f8b3f9d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/EEG_analysis/cnn_lstm')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from model import LSTMClassifier\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import os\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve, auc, classification_report\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oB4NDi6sW7Tv"
      },
      "outputs": [],
      "source": [
        "class SeizureDataset(Dataset):\n",
        "    def __init__(self, data_path, label_path):\n",
        "        # Load data and labels\n",
        "        self.data = pd.read_csv(data_path, header=None).values\n",
        "        self.labels = pd.read_csv(label_path, header=None).values\n",
        "\n",
        "        # Only use features from index 934 to 1279\n",
        "        self.feature_start = 768\n",
        "        self.feature_end = 1280\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Extract the relevant features (934-1279)\n",
        "        features = self.data[idx, self.feature_start:self.feature_end]\n",
        "        # Convert to tensor and reshape to [sequence_length, num_features]\n",
        "        # We'll treat the 346 features as 346 timesteps with 1 feature each\n",
        "        features = torch.FloatTensor(features).unsqueeze(1)  # [346, 1]\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return features, label\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    data, labels = zip(*batch)\n",
        "    data_padded = pad_sequence(data, batch_first=True, padding_value=0)\n",
        "    labels = torch.stack(labels, dim=0)\n",
        "\n",
        "    return data_padded, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHgpSfVTM0GW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import math\n",
        "\n",
        "class LSTMClassifier1D(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_rate=0.5):\n",
        "        super(LSTMClassifier1D, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Conv1d: expecting input shape (batch, channels=1, seq_len)\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
        "        self.conv2 = nn.Conv1d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(hidden_size + input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)\n",
        "\n",
        "        # FC layers\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc1 = nn.Linear(hidden_size, 256)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn4 = nn.BatchNorm1d(128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, seq_len, input_size)\n",
        "        batch_size, seq_len, input_size = x.size()\n",
        "\n",
        "        # Permute to (batch, input_size, seq_len) for Conv1d\n",
        "        x_conv = x.permute(0, 2, 1)\n",
        "        x_conv = torch.relu(self.bn1(self.conv1(x_conv)))\n",
        "        x_conv = torch.relu(self.bn2(self.conv2(x_conv)))  # Shape: (batch, hidden_size, seq_len)\n",
        "\n",
        "        x_conv = x_conv.permute(0, 2, 1)  # Back to (batch, seq_len, hidden_size)\n",
        "\n",
        "        # Residual connection (original input): (batch, seq_len, input_size)\n",
        "        x_cat = torch.cat((x_conv, x), dim=2)  # (batch, seq_len, hidden_size + input_size)\n",
        "\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
        "\n",
        "        out, _ = self.lstm(x_cat, (h0, c0))\n",
        "        out = out[:, -1, :]  # Take last time step\n",
        "\n",
        "        out = self.dropout(out)\n",
        "        out = torch.relu(self.bn3(self.fc1(out)))\n",
        "        out = self.dropout(out)\n",
        "        out = torch.relu(self.bn4(self.fc2(out)))\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhvUJN-6W7Tv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define paths\n",
        "data_path = '/content/drive/MyDrive/Colab Notebooks/EEG_analysis/processed_data/'\n",
        "train_features_path = data_path + \"concatenated_train_final_data_epoch_final.csv\"\n",
        "train_labels_path = data_path + \"train_labels.csv\"\n",
        "test_features_path = data_path + \"concatenated_test_final_data_epoch_final.csv\"\n",
        "test_labels_path = data_path + \"test_labels.csv\"\n",
        "\n",
        "# Initialize datasets\n",
        "train_dataset = SeizureDataset(train_features_path, train_labels_path)\n",
        "test_dataset = SeizureDataset(test_features_path, test_labels_path)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) # or 64, 128, 256\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CxBgGz1W7Tw",
        "outputId": "65722c5f-4588-4c63-cbf2-f00bcf7acb28"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = LSTMClassifier1D(\n",
        "    input_size=1,      # 1 channel EEG\n",
        "    hidden_size=128,   # Hidden layer size\n",
        "    num_layers=4,      # Number of LSTM layers\n",
        "    num_classes=1,     # Binary classification\n",
        "    dropout_rate=0.2\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "tolerance_counter = 0\n",
        "\n",
        "epochs = 150\n",
        "tolerance = 5  # Number of epochs to tolerate non-decreasing val loss before switching dataloader\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    t_loss = 0\n",
        "    for data, ground_truth in tqdm(train_loader, leave=False):\n",
        "        data, ground_truth = data.to(device), ground_truth.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(data)\n",
        "        ground_truth = ground_truth.view(-1, 1)\n",
        "        ground_truth = ground_truth.float()\n",
        "        loss = criterion(pred, ground_truth)\n",
        "        t_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print average loss for the epoch (moved outside the batch loop)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {t_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        torch.save(model.state_dict(), f'ckpts/weight_lstm_{epoch}.pth')\n",
        "\n",
        "print(\"Training complete!\")\n",
        "torch.save(model.state_dict(), 'ckpts/weight_lstm_final.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_lN01xPW7Tw"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "kzHpsQV-W7Tx",
        "outputId": "134fae89-70f5-4503-fec2-1fa27dfe9d07"
      },
      "outputs": [],
      "source": [
        "# Generate predictions\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        preds = torch.sigmoid(model(data))  # Convert logits to probabilities\n",
        "        preds = (preds > 0.5).int()  # Threshold at 0.5\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Non-Seizure\", \"Seizure\"],\n",
        "            yticklabels=[\"Non-Seizure\", \"Seizure\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "iaHx4ye5W7Tx",
        "outputId": "3b69df25-c1a3-4b70-8a97-335c466ccb53"
      },
      "outputs": [],
      "source": [
        "# Get predicted probabilities (not thresholded)\n",
        "all_probs = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        probs = torch.sigmoid(model(data)).cpu().numpy()\n",
        "        all_probs.extend(probs)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
        "auc_score = roc_auc_score(all_labels, all_probs)\n",
        "\n",
        "# Plot\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRmPozEGW7Tx"
      },
      "outputs": [],
      "source": [
        "# Get some test samples\n",
        "test_samples, test_labels = next(iter(test_loader))\n",
        "test_samples = test_samples.to(device)\n",
        "\n",
        "# Get predictions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = torch.sigmoid(model(test_samples))\n",
        "    preds = (outputs > 0.5).float()\n",
        "\n",
        "# Visualize first 5 samples\n",
        "for i in range(5):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(test_samples[i].cpu().numpy())\n",
        "    plt.title(f\"True: {test_labels[i].item()}, Pred: {preds[i].item()}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yZ7GPMVW7Tx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUhl3W6FW7Tx"
      },
      "outputs": [],
      "source": [
        "\n",
        "precision, recall, _ = precision_recall_curve(all_labels, all_probs)\n",
        "auprc = auc(recall, precision)\n",
        "\n",
        "plt.plot(recall, precision, label=f\"AUPRC = {auprc:.2f}\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHUGyRihW7Tx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUAfa9NNW7Tx"
      },
      "outputs": [],
      "source": [
        "# Select a sample from the test set\n",
        "sample_idx = 0  # Change to visualize different samples\n",
        "data, label = test_dataset[sample_idx]\n",
        "\n",
        "# Generate prediction\n",
        "with torch.no_grad():\n",
        "    data = data.unsqueeze(0).to(device)  # Add batch dimension\n",
        "    prob = torch.sigmoid(model(data)).item()\n",
        "pred = \"Seizure\" if prob > 0.5 else \"Non-Seizure\"\n",
        "\n",
        "# Plot the time-series data\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(data.cpu().numpy()[:, 0], label=\"EEG Channel 1\")  # Plot 1st feature\n",
        "plt.title(f\"True Label: {label.item()} | Predicted: {pred} (Prob: {prob:.2f})\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjLtbDv0W7Tx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFdXRn2EW7Tx"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(classification_report(all_labels, all_preds,\n",
        "                            target_names=[\"Non-Seizure\", \"Seizure\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AmwX8Q4W7Tx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
