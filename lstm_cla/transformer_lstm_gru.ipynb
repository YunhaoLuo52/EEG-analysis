{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Device configuration\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Hyperparameters\u001b[39;00m\n\u001b[1;32m      5\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 100\n",
    "INPUT_SIZE = 1  # Each timestep has 1 feature\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_LAYERS = 3\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.25\n",
    "FEATURE_START = 934  # Start index of features to use\n",
    "FEATURE_END = 1280   # End index of features to use\n",
    "\n",
    "# Custom Dataset Class\n",
    "class SeizureDataset(Dataset):\n",
    "    def __init__(self, features, labels, transform=None):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get EEG segment and apply STFT if transform is specified\n",
    "        eeg_segment = self.features[idx]\n",
    "        if self.transform:\n",
    "            eeg_segment = self.transform(eeg_segment)\n",
    "        \n",
    "        # Convert to tensor and reshape to [seq_len, 1]\n",
    "        eeg_tensor = torch.FloatTensor(eeg_segment).unsqueeze(1)\n",
    "        label_tensor = torch.FloatTensor([self.labels[idx]])\n",
    "        \n",
    "        return eeg_tensor, label_tensor\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT Transform\n",
    "def apply_stft(eeg_segment):\n",
    "    f, t, Zxx = signal.stft(eeg_segment, fs=173.61, nperseg=256)\n",
    "    # Take magnitude and log transform\n",
    "    stft = np.log(np.abs(Zxx) + 1e-8)\n",
    "    # Flatten the time-frequency representation\n",
    "    return stft.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Encoder Layer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size * 4, hidden_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Self attention\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Feed forward\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Model Architecture\n",
    "class SeizurePredictionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Time and Frequency Encoders (Transformers)\n",
    "        self.time_encoder = nn.Sequential(\n",
    "            *[TransformerEncoderLayer(HIDDEN_SIZE, NUM_HEADS, DROPOUT) for _ in range(2)]\n",
    "        )\n",
    "        self.freq_encoder = nn.Sequential(\n",
    "            *[TransformerEncoderLayer(HIDDEN_SIZE, NUM_HEADS, DROPOUT) for _ in range(2)]\n",
    "        )\n",
    "        \n",
    "        # Recurrent Networks\n",
    "        self.lstm = nn.LSTM(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, \n",
    "                           batch_first=True, dropout=DROPOUT)\n",
    "        self.gru = nn.GRU(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS,\n",
    "                         batch_first=True, dropout=DROPOUT)\n",
    "        \n",
    "        # Gating Mechanism\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_SIZE * 2, HIDDEN_SIZE),\n",
    "            nn.Softmax(dim=1)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(HIDDEN_SIZE * 2, HIDDEN_SIZE),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROPOUT),\n",
    "            nn.Linear(HIDDEN_SIZE, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, 1]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Time pathway (LSTM)\n",
    "        lstm_out, _ = self.lstm(x)  # [batch_size, seq_len, hidden_size]\n",
    "        lstm_last = lstm_out[:, -1, :]  # Last timestep\n",
    "        \n",
    "        # Frequency pathway (GRU)\n",
    "        gru_out, _ = self.gru(x)  # [batch_size, seq_len, hidden_size]\n",
    "        gru_last = gru_out[:, -1, :]  # Last timestep\n",
    "        \n",
    "        # Transformer encoders\n",
    "        time_encoded = self.time_encoder(lstm_out)  # [batch_size, seq_len, hidden_size]\n",
    "        freq_encoded = self.freq_encoder(gru_out)   # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Take last timestep from transformer outputs\n",
    "        time_last = time_encoded[:, -1, :]\n",
    "        freq_last = freq_encoded[:, -1, :]\n",
    "        \n",
    "        # Gating mechanism\n",
    "        combined = torch.cat([time_last, freq_last], dim=1)\n",
    "        gates = self.gate(combined)\n",
    "        gate1, gate2 = gates.chunk(2, dim=1)\n",
    "        \n",
    "        # Weighted features\n",
    "        weighted_time = time_last * gate1\n",
    "        weighted_freq = freq_last * gate2\n",
    "        final_features = torch.cat([weighted_time, weighted_freq], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(final_features)\n",
    "        \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "def load_data(features_path, labels_path, test_size=0.2):\n",
    "    # Load features and labels\n",
    "    features = pd.read_csv(features_path, header=None).values\n",
    "    labels = pd.read_csv(labels_path, header=None).values.flatten()\n",
    "    \n",
    "    # Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=test_size, random_state=42, stratify=labels)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    return train_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Non-Seizure', 'Seizure']))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, ['Non-Seizure', 'Seizure'])\n",
    "    plt.yticks(tick_marks, ['Non-Seizure', 'Seizure'])\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your data\n",
    "X_train, X_test, y_train, y_test = load_data('train.csv', 'train_label.csv')\n",
    "    \n",
    "# Create datasets with STFT transform\n",
    "train_dataset = SeizureDataset(X_train, y_train, transform=apply_stft)\n",
    "test_dataset = SeizureDataset(X_test, y_test, transform=apply_stft)\n",
    "    \n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "# Initialize model\n",
    "model = SeizurePredictionModel().to(device)\n",
    "    \n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_losses = train_model(model, train_loader, criterion, optimizer, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "    \n",
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader)\n",
    "    \n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'seizure_prediction_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEGDiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
